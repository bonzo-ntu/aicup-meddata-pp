{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/fuck/aicup-meddata-pp_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "api_key = \"<your OpenAI api key>\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate finetune file and create tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "  file=open('prompt1.jsonl', 'rb'),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"<the file id from prompt1.jsonl>\", \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix=\"prompt1\",\n",
    "  hyperparameters={\"n_epochs\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "  file=open('prompt2.jsonl', 'rb'),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"<the file id from prompt2.jsonl>\", \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix=\"prompt2\",\n",
    "  hyperparameters={\"n_epochs\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "  file=open('prompt3.jsonl', 'rb'),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"<the file id from prompt3.jsonl>\", \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix=\"prompt3\",\n",
    "  hyperparameters={\"n_epochs\":10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))\n",
    "def label(record, sentence, model=\"gpt-3.5-turbo\", prompt_type = 3):\n",
    "\n",
    "    if is_SET_sentence(sentence):\n",
    "        ths = {'twice':'R2', 'third':'R3', 'fourth':'R4', 'fifth':'R5', 'sixth':'R6', 'seventh':'R7', 'eighth':'R8', 'ninth':'R9', 'tenth':'R10'}\n",
    "        response = []\n",
    "        for key, value in ths.items():\n",
    "            if key in sentence:\n",
    "                response.append(f'SET:{key}=>{value}')\n",
    "        response = '\\\\n'.join(response)\n",
    "        return response\n",
    "        \n",
    "    elif len(sentence.lstrip().rstrip()) < 4:\n",
    "        return 'PHI:Null'\n",
    "\n",
    "    else:\n",
    "        role = \"You are a doctor, capable of identifying Protected Health Information (PHI) within medical records.\"\n",
    "\n",
    "        if prompt_type == 1:\n",
    "            phis = \"PATIENT, DOCTOR, ROOM, DEPARTMENT, HOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP, LOCATION-OTHER, AGE, DATE, TIME, DURATION, SET, PHONE, URL, MEDICALRECORD, IDNUM\"\n",
    "            rule = f\"PHI categories are: {phis}. If the content can not be identified as any of PHI category, reply a special string 'PHI:Null'\"\n",
    "            user_input = f\"Please read the medical record: ```\\n{record[:3659]}\\n``` and identify all PHI categories in the the sentence: ```\\n{sentence}\\n```. Respond in the format 'PHI Category: PHI Content' for each identified PHI category. If there are multiple PHI categories in the sentence, separate all 'PHI Category: PHI Content' pairs with '\\n'.\"\n",
    "\n",
    "        elif prompt_type == 2:\n",
    "            phis = \"PATIENT, DOCTOR, DEPARTMENT, HOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP, AGE, DATE, TIME, MEDICALRECORD, IDNUM\"\n",
    "            rule = f\"PHI categories are: {phis}. If the content can not be identified as any of PHI category, reply a special string 'PHI:Null'\"\n",
    "            user_input = f\"Please read the medical record paragraph: ```\\n{record}\\n``` and identify all PHI categories in this sentence: ```\\n{sentence}\\n```. Respond in the format 'PHI Category: PHI Content' for each identified PHI category. If there are multiple PHI categories in the sentence, separate all 'PHI Category: PHI Content' pairs with '\\n'.\"\n",
    "\n",
    "        elif prompt_type ==3:\n",
    "            phis = \"PATIENT, DOCTOR, DEPARTMENT, HOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP, AGE, DATE, TIME, MEDICALRECORD, IDNUM\"\n",
    "            rule = f\"PHI categories are: {phis}. If the content can not be identified as any of PHI category, reply a special string 'PHI:Null'\"\n",
    "            user_input = f\"Please identify all PHI categories in this sentence (from a medical record): ```\\n{sentence}\\n```. Respond in the format 'PHI Category:PHI Content' for each identified PHI category. If multiple PHI categories are identified, concatenate 'PHI Category:PHI Content' pairs with '\\n'.\"\n",
    "\n",
    "\n",
    "\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": role},\n",
    "            {\"role\": \"assistant\", \"content\": rule},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "\n",
    "        tokens_remain = 4096 - num_tokens_from_string(user_input)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=conversation,\n",
    "            max_tokens=(256 if tokens_remain >= 256 else tokens_remain)\n",
    "        )\n",
    "\n",
    "\n",
    "        r = response.choices[0].message.content\n",
    "\n",
    "        if 'SET:' in r:\n",
    "            r = 'PHI:Null'\n",
    "\n",
    "        elif 'AGE:' in r:\n",
    "            if not r.replace('AGE:','').isdigit():\n",
    "                r = 'PHI:Null'\n",
    "\n",
    "        elif len(r) < 1:\n",
    "            r = 'PHI:Null'\n",
    "        elif 'PHI:' in r and len(r.replace('PHI:','')) < 2:\n",
    "            r = 'PHI:Null'\n",
    "\n",
    "        return r\n",
    "\n",
    "\n",
    "\n",
    "def is_SET_sentence(sentence):\n",
    "    import re\n",
    "    pattern1 = r'\\b(?:examined|repeated|tested|re-examined|reexamined|evaluated|assessed|verified|scrutinized|proven|inspected|surveyed|investigated|analyzed|reviewed|reassessed|reevaluated|retested|reworked|iterated|recurred|replicated|duplicated|recycled)\\b\\s\\b(?:twice|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\\b'\n",
    "    pattern2 = r'\\b(?:examined|repeated|tested|re-examined|reexamined|evaluated|assessed|verified|scrutinized|proven|inspected|surveyed|investigated|analyzed|reviewed|reassessed|reevaluated|retested|reworked|iterated|recurred|replicated|duplicated|recycled)\\b\\s\\(\\b(?:twice|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\\b\\)'\n",
    "\n",
    "    match = re.search(pattern1, sentence, re.IGNORECASE) or re.search(pattern2, sentence, re.IGNORECASE)\n",
    "\n",
    "    return True if match else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import time\n",
    "import random\n",
    "\n",
    "valid = pd.read_csv('./test_pp_context.tsv', delimiter='\\t').fillna('')\n",
    "\n",
    "model, prompt_type, output_file = '<the model id fintuned by prompt1.jsonl>', 1, 'prompt2_result'\n",
    "model, prompt_type, output_file = '<the model id fintuned by prompt2.jsonl>', 2, 'prompt2_result'\n",
    "model, prompt_type, output_file = '<the model id fintuned by prompt3.jsonl>', 3, 'prompt3_result'\n",
    "\n",
    "labels = []\n",
    "\n",
    "\n",
    "i = 0\n",
    "for id, row in tqdm(valid.iterrows()):\n",
    "    file, sentence = row.file, row.sentence\n",
    "    record_path = './content/First_Phase_Release/Validation_Release'\n",
    "    if prompt_type == 1:\n",
    "        record = open(f'{record_path}/{file}.txt', 'r').read()\n",
    "    else:\n",
    "        record = row.context\n",
    "    response = label(record, sentence, model, prompt_type)\n",
    "    labels.append(response)\n",
    "    i += 1\n",
    "\n",
    "valid['answer'] = labels     \n",
    "print('labels finish! len=', len(labels))\n",
    "\n",
    "\n",
    "files = []\n",
    "phis = []\n",
    "start_ids = []\n",
    "end_ids = []\n",
    "contents = []\n",
    "for id, row in valid.iterrows():\n",
    "    file, start_id, sentence, answer = row.file, row.start_id, row.sentence, row.answer\n",
    "    for ans in answer.split('\\\\n'):\n",
    "        if ans == 'PHI:Null':\n",
    "            continue\n",
    "\n",
    "        phi = ans.split(':')[0]\n",
    "        content = ':'.join(ans.split(':')[1:])\n",
    "        content = content.lstrip().rstrip()\n",
    "\n",
    "        if content == 'Null' or (not content.isdigit() and len(content)==1):\n",
    "            continue\n",
    "\n",
    "        if phi == 'AGE' and (content.isdigit() and not 0<int(content) <100):\n",
    "            continue\n",
    "\n",
    "\n",
    "        content2=''\n",
    "        if phi in [\"DATE\", \"TIME\", \"DURATION\", \"SET\"]:\n",
    "            content_tmp = content\n",
    "            content = content_tmp.split('=>')[0]\n",
    "            content2 = ''.join(content_tmp.split('=>')[1:])\n",
    "\n",
    "        anchor = start_id\n",
    "        offset = sentence.find(content)\n",
    "        start_id = start_id+offset\n",
    "        end_id = start_id + len(content)\n",
    "\n",
    "        content = content+'<<<SEP>>>'+content2 if content2 else content\n",
    "\n",
    "        if len(content) > 0 and phi in [\"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"ROOM\", \"DEPARTMENT\", \"HOSPITAL\", \"ORGANIZATION\", \"STREET\", \"CITY\", \"STATE\", \"COUNTRY\", \"ZIP\", \"LOCATION-OTHER\", \"AGE\", \"DATE\", \"TIME\", \"DURATION\", \"SET\", \"PHONE\", \"FAX\", \"EMAIL\", \"URL\", \"IPADDR\", \"SSN\", \"MEDICALRECORD\", \"HEALTHPLAN\", \"ACCOUNT\", \"LICENSE\", \"VEHICLE\", \"DEVICE\", \"BIOID\", \"IDNUM\"]:\n",
    "            files.append(file)\n",
    "            phis.append(phi)\n",
    "            start_ids.append(start_id)\n",
    "            end_ids.append(end_id)\n",
    "            contents.append(content)\n",
    "        start_id = end_id\n",
    "        sentence = sentence[offset+len(content):]\n",
    "        \n",
    "        \n",
    "\n",
    "answers = pd.DataFrame({'file':files, 'phi':phis, 'start_id':start_ids, 'end_id':end_ids, 'content':contents})\n",
    "answers.to_csv(output_file, header=False, index=False, sep='\\t')\n",
    "\n",
    "with open(output_file, 'r') as r, open(output_file+'.out', 'w') as w:\n",
    "    for rline in r:\n",
    "        if rline.split('\\t')[1] in [\"DATE\", \"TIME\", \"DURATION\", \"SET\"]:\n",
    "            rline = rline.replace('<<<SEP>>>', '\\t')\n",
    "        w.write(rline)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
