{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/aicup-meddata-pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context(df, valid=False):\n",
    "    source_folder_dict = {1:'./content/First_Phase_Release/First_Phase_Text_Dataset', \n",
    "                          2:'./content/Second_Phase_Dataset/Second_Phase_Text_Dataset',\n",
    "                          3:'./content/opendid_test/', \n",
    "                          4:'./content/First_Phase_Release/Validation_Release'} \n",
    "    \n",
    "    current_file = ''\n",
    "    \n",
    "    context = []\n",
    "    for id, row in df.iterrows():\n",
    "        \n",
    "        file, start_id, sentence =  row.file, row.start_id, row.sentence\n",
    "        if not valid:\n",
    "            source =  row.source\n",
    "            record_path = source_folder_dict[source]\n",
    "        else:\n",
    "            record_path = './content/First_Phase_Release/Validation_Release'\n",
    "        new_file = f'{record_path}/{file}.txt'\n",
    "        if current_file != new_file:\n",
    "            current_file = new_file\n",
    "            record = open(current_file, 'r').read()\n",
    "\n",
    "        s = 0 if start_id - 100 <= 0 else start_id - 100\n",
    "        e = start_id + len(sentence) + 100\n",
    "        context.append(record[s:e])\n",
    "\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_pp.tsv', delimiter='\\t')\n",
    "valid = pd.read_csv('./valid_pp.tsv', delimiter='\\t')\n",
    "test = pd.read_csv('./test_pp.tsv', delimiter='\\t')\n",
    "\n",
    "train['sentence'] = train.sentence.fillna('')\n",
    "valid['sentence'] = valid.sentence.fillna('')\n",
    "test['sentence'] = test.sentence.fillna('')\n",
    "\n",
    "train['context'] = find_context(train)\n",
    "valid['context'] = find_context(valid)\n",
    "test['context'] = find_context(test)\n",
    "\n",
    "train.to_csv('./train_pp_context.tsv', index=False, sep='\\t')\n",
    "valid.to_csv('./valid_pp_context.tsv', index=False, sep='\\t')\n",
    "test.to_csv('./test_pp_context.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phis(s):\n",
    "    return [section.split(':')[0] for section in s.split('\\\\n') if section.split(':')[0]]\n",
    "\n",
    "def gen_ft_df(start = 0, n = 20, skip_low=False, threshold=136):\n",
    "    df = pd.read_csv('./train_pp_context.tsv', delimiter='\\t').fillna('')\n",
    "\n",
    "    phis = \"PATIENT, DOCTOR, DEPARTMENT, HOSPITAL, STREET, CITY, STATE, ZIP, DATE, TIME, MEDICALRECORD, IDNUM, PHI\".split(',')\n",
    "    phis = [_.replace(' ','') for _ in phis]\n",
    "\n",
    "    train_df_data, train_df_meta = {}, {}\n",
    "    for phi in phis:\n",
    "        result = df[df.label.apply(lambda x: phi in get_phis(x))].copy()\n",
    "        if result.shape[0] > 0:\n",
    "            train_df_data[phi] = result\n",
    "            train_df_meta[phi] = result.shape[0]\n",
    "        else:\n",
    "            print(f'{phi} skipped')\n",
    "    \n",
    "\n",
    "    dfs = []\n",
    "    for phi in phis:\n",
    "        if train_df_meta[phi] < threshold:\n",
    "            if not skip_low:\n",
    "                dfs.append(train_df_data[phi])\n",
    "        else:\n",
    "            tmp = train_df_data[phi]\n",
    "            tmp1 = tmp[tmp.source == 1].iloc[start:start+n//2, :]\n",
    "            tmp2 = tmp[tmp.source == 2].iloc[start:start+n//2, :]\n",
    "            dfs.append(tmp1)\n",
    "            dfs.append(tmp2)\n",
    "\n",
    "    ft_df = pd.concat(dfs, axis=0).reset_index(drop=True).drop_duplicates()\n",
    "    return ft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "def gen_ft_file(df, filename, prompt_type = 3):\n",
    "    source_folder_dict = {1:'./content/First_Phase_Release/First_Phase_Text_Dataset', \n",
    "                          2:'./content/Second_Phase_Dataset/Second_Phase_Text_Dataset',\n",
    "                          3:'./content/opendid_test/', \n",
    "                          4:'./content/First_Phase_Release/Validation_Release'} \n",
    "    \n",
    "    with open(filename, 'w') as jsonl_file:\n",
    "        for row in df.iterrows():\n",
    "            id, row = row\n",
    "\n",
    "            role = \"You are a doctor, capable of identifying Protected Health Information (PHI) within medical records.\"\n",
    "\n",
    "            source, file, sentence, label =  row.source, row.file, row.sentence, row.label\n",
    "            record_path = source_folder_dict[source]\n",
    "\n",
    "            if prompt_type == 1: \n",
    "                record = open(f'{record_path}/{file}.txt', 'r').read()\n",
    "\n",
    "                phis = \"PATIENT, DOCTOR, ROOM, DEPARTMENT, HOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP, LOCATION-OTHER, AGE, DATE, TIME, DURATION, SET, PHONE, URL, MEDICALRECORD, IDNUM\"\n",
    "                rule = f\"PHI categories are: {phis}. If the content can not be identified as any of PHI category, reply a special string 'PHI:Null'\"\n",
    "                user_input = f\"Please read the medical record: ```\\n{record}\\n``` and identify all PHI categories in the the sentence: ```\\n{sentence}\\n```. Respond in the format 'PHI Category: PHI Content' for each identified PHI category. If there are multiple PHI categories in the sentence, separate all 'PHI Category: PHI Content' pairs with '\\n'.\"\n",
    "            elif prompt_type == 2:\n",
    "                record = row.context\n",
    "\n",
    "                phis = \"PATIENT, DOCTOR, DEPARTMENT, HOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP, AGE, DATE, TIME, MEDICALRECORD, IDNUM\"\n",
    "                rule = f\"PHI categories are: {phis}. If the content can not be identified as any of PHI category, reply a special string 'PHI:Null'\"\n",
    "                user_input = f\"Please read the medical record paragraph: ```\\n{record}\\n``` and identify all PHI categories in this sentence: ```\\n{sentence}\\n```. Respond in the format 'PHI Category: PHI Content' for each identified PHI category. If there are multiple PHI categories in the sentence, separate all 'PHI Category: PHI Content' pairs with '\\n'.\"\n",
    "            elif prompt_type == 3:\n",
    "                phis = \"PATIENT, DOCTOR, DEPARTMENT, HOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP, AGE, DATE, TIME, MEDICALRECORD, IDNUM\"\n",
    "                rule = f\"PHI categories are: {phis}. If the content can not be identified as any of PHI category, reply a special string 'PHI:Null'\"\n",
    "                user_input = f\"Please identify all PHI categories in this sentence (from a medical record): ```\\n{sentence}\\n```. Respond in the format 'PHI Category:PHI Content' for each identified PHI category. If multiple PHI categories are identified, concatenate 'PHI Category:PHI Content' pairs with '\\n'.\"\n",
    "\n",
    "\n",
    "            ft_sample = [\n",
    "                {\"role\": \"system\", \"content\": role},\n",
    "                {\"role\": \"assistant\", \"content\": rule},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": label}\n",
    "            ]\n",
    "\n",
    "            item = {\"messages\":ft_sample}    \n",
    "            jsonl_file.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ft_file(gen_ft_df(0, n=100, skip_low=False), 'prompt1.jsonl', prompt_type=1)\n",
    "gen_ft_file(gen_ft_df(0, n=100, skip_low=True), 'prompt2.jsonl', prompt_type=2)\n",
    "gen_ft_file(gen_ft_df(0, n=500, skip_low=True), 'prompt3.jsonl', prompt_type=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
