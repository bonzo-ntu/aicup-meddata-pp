{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir('/home/jupyter/aicup-meddata-pp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解壓縮官網下載之資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p content\n",
    "!yes | unzip ./content/First_Phase_ReleaseCorrection.zip -d ./content\n",
    "!yes | unzip ./content/Second_Phase_Dataset.zip -d ./content\n",
    "!yes | unzip ./content/Validation_Dataset_Answer.zip -d ./content\n",
    "!yes | unzip ./content/opendid_test.zip -d ./content\n",
    "\n",
    "!mv ./content/First_Phase_Release\\(Correction\\) ./content/First_Phase_Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path , 'r' , encoding = 'utf-8-sig') as fr:\n",
    "        return fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos = '<|endoftext|>'\n",
    "eos = '<|END|>'\n",
    "pad = '<|pad|>'\n",
    "ner = '\\n\\n####\\n\\n'\n",
    "special_tokens_dict = {'bos_token': bos,\n",
    "                       'eos_token': eos,\n",
    "                       'pad_token': pad,\n",
    "                       'sep_token': ner}\n",
    "\n",
    "def process_annotation_file(lines):\n",
    "    '''\n",
    "    處理anwser.txt 標註檔案\n",
    "\n",
    "    output:annotation dicitonary\n",
    "    '''\n",
    "    print(\"process annotation file...\")\n",
    "    entity_dict = {}\n",
    "    for line in lines:\n",
    "        items = line.strip('\\n').split('\\t')\n",
    "        if len(items) == 5:\n",
    "            item_dict = {\n",
    "                'phi' : items[1],\n",
    "                'st_idx' : int(items[2]),\n",
    "                'ed_idx' : int(items[3]),\n",
    "                'entity' : items[4],\n",
    "            }\n",
    "        elif len(items) == 6:\n",
    "            item_dict = {\n",
    "                'phi' : items[1],\n",
    "                'st_idx' : int(items[2]),\n",
    "                'ed_idx' : int(items[3]),\n",
    "                'entity' : items[4],\n",
    "                'normalize_time' : items[5],\n",
    "            }\n",
    "        if items[0] not in entity_dict:\n",
    "            entity_dict[items[0]] = [item_dict]\n",
    "        else:\n",
    "            entity_dict[items[0]].append(item_dict)\n",
    "    print(\"annotation file done\")\n",
    "    return entity_dict\n",
    "\n",
    "def process_medical_report(txt_name, medical_report_folder, annos_dict, special_tokens_dict):\n",
    "    '''\n",
    "    處理單個病理報告\n",
    "\n",
    "    output : 處理完的 sequence pairs\n",
    "    '''\n",
    "    file_name = txt_name + '.txt'\n",
    "    sents = read_file(os.path.join(medical_report_folder, file_name))\n",
    "    article = \"\".join(sents)\n",
    "\n",
    "    bounary , item_idx , temp_seq , seq_pairs = 0 , 0 , \"\" , []\n",
    "    new_line_idx = 0\n",
    "    for w_idx, word in enumerate(article):\n",
    "        if word == '\\n':\n",
    "            new_line_idx = w_idx + 1\n",
    "            if article[bounary:new_line_idx] == '\\n':\n",
    "                continue\n",
    "            if temp_seq == \"\":\n",
    "                temp_seq = \"PHI:Null\"\n",
    "            sentence = article[bounary:new_line_idx].strip().replace('\\t' , ' ')\n",
    "            temp_seq = temp_seq.strip('\\\\n')\n",
    "            seq_pair = f\"{txt_name}\\t{new_line_idx}\\t{sentence}\\t{temp_seq}\\n\"\n",
    "            # seq_pair = special_tokens_dict['bos_token'] + article[bounary:new_line_idx] + special_tokens_dict['sep_token'] + temp_seq + special_tokens_dict['eos_token']\n",
    "            bounary = new_line_idx\n",
    "            seq_pairs.append(seq_pair)\n",
    "            temp_seq = \"\"\n",
    "        if w_idx == annos_dict[txt_name][item_idx]['st_idx']:\n",
    "            phi_key = annos_dict[txt_name][item_idx]['phi']\n",
    "            phi_value = annos_dict[txt_name][item_idx]['entity']\n",
    "            if 'normalize_time' in annos_dict[txt_name][item_idx]:\n",
    "                temp_seq += f\"{phi_key}:{phi_value}=>{annos_dict[txt_name][item_idx]['normalize_time']}\\\\n\"\n",
    "            else:\n",
    "                temp_seq += f\"{phi_key}:{phi_value}\\\\n\"\n",
    "            if item_idx == len(annos_dict[txt_name]) - 1:\n",
    "                continue\n",
    "            item_idx += 1\n",
    "    return seq_pairs\n",
    "\n",
    "def generate_annotated_medical_report_parallel(anno_file_path, medical_report_folder , tsv_output_path , num_processes=4):\n",
    "    '''\n",
    "    呼叫上面的兩個function\n",
    "    處理全部的病理報告和標記檔案\n",
    "\n",
    "    output : 全部的 sequence pairs\n",
    "    '''\n",
    "    anno_lines = read_file(anno_file_path)\n",
    "    annos_dict = process_annotation_file(anno_lines)\n",
    "    txt_names = list(annos_dict.keys())\n",
    "\n",
    "    print(\"processing each medical file\")\n",
    "\n",
    "    all_seq_pairs = []\n",
    "    for txt_name in txt_names:\n",
    "        all_seq_pairs.extend(process_medical_report(txt_name, medical_report_folder, annos_dict, special_tokens_dict))\n",
    "    print(all_seq_pairs[:10])\n",
    "    print(\"All medical file done\")\n",
    "    print(\"write out to tsv format...\")\n",
    "    with open(tsv_output_path , 'w' , encoding = 'utf-8') as fw:\n",
    "        for seq_pair in all_seq_pairs:\n",
    "            fw.write(seq_pair)\n",
    "    print(\"tsv format dataset done\")\n",
    "    # return all_seq_pairs\n",
    "\n",
    "\n",
    "def process_valid_data(test_txts , out_file):\n",
    "    with open(out_file , 'w' , encoding = 'utf-8') as fw:\n",
    "        for txt in test_txts:\n",
    "            m_report = read_file(txt)\n",
    "            boundary = 0\n",
    "            # temp = ''.join(m_report)\n",
    "            fid = txt.split('/')[-1].replace('.txt' , '')\n",
    "            for idx,sent in enumerate(m_report):\n",
    "                #sent = sent[:-1] if sent[-1] == '\\n' else sent\n",
    "                if sent.replace(' ' , '').replace('\\n' , '').replace('\\t' , '') != '':\n",
    "                    sent = sent.replace('\\t' , ' ')\n",
    "                    newline = '' if sent[-1] == '\\n' else '\\n'\n",
    "                    fw.write(f\"{fid}\\t{boundary}\\t{sent}{newline}\")\n",
    "                # else:\n",
    "                #     print(f\"{fid}\\t{boundary}\\t{sent}\\n\")\n",
    "                #     assert 1==2\n",
    "                boundary += len(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用官方提供程式碼將資料集前處理成 .tsv 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process annotation file...\n",
      "annotation file done\n",
      "processing each medical file\n",
      "['10\\t25\\tEpisode No:  09F016547J\\tIDNUM:09F016547J\\n', '10\\t36\\t091016.NMT\\tMEDICALRECORD:091016.NMT\\n', '10\\t52\\tSIZAR, HOWARD\\tPATIENT:SIZAR, HOWARD\\n', '10\\t70\\tLab No:  09F01654\\tIDNUM:09F01654\\n', '10\\t78\\tRunford\\tSTREET:Runford\\n', '10\\t97\\tRENMARK  TAS  5084\\tCITY:RENMARK\\\\nSTATE:TAS\\\\nZIP:5084\\n', '10\\t114\\tSpecimen: Tissue\\tPHI:Null\\n', '10\\t132\\tD.O.B:  24/8/1993\\tDATE:24/8/1993=>1993-08-24\\n', '10\\t140\\tSex:  M\\tPHI:Null\\n', '10\\t171\\tCollected: 28/08/2013 at 08:26\\tTIME:28/08/2013 at 08:26=>2013-08-28T08:26\\n']\n",
      "All medical file done\n",
      "write out to tsv format...\n",
      "tsv format dataset done\n",
      "process annotation file...\n",
      "annotation file done\n",
      "processing each medical file\n",
      "['1093\\t25\\tEpisode No:  48B915480A\\tIDNUM:48B915480A\\n', '1093\\t37\\t4809154.WAA\\tMEDICALRECORD:4809154.WAA\\n', '1093\\t58\\tOtterbine, Laverne\\tPATIENT:Otterbine, Laverne\\n', '1093\\t85\\tLab No:  48B91548,48B91548\\tIDNUM:48B91548\\\\nIDNUM:48B91548\\n', '1093\\t98\\tLegend Manor\\tSTREET:Legend Manor\\n', '1093\\t122\\tNORTHAM  Tasmania  2207\\tCITY:NORTHAM\\\\nSTATE:Tasmania\\\\nZIP:2207\\n', '1093\\t148\\tSpecimen: Washings,Tissue\\tPHI:Null\\n', '1093\\t166\\tD.O.B:  20/6/1989\\tDATE:20/6/1989=>1989-06-20\\n', '1093\\t174\\tSex:  F\\tPHI:Null\\n', '1093\\t200\\tCollected: 28/8/2063 at :\\tDATE:28/8/2063=>2063-08-28\\n']\n",
      "All medical file done\n",
      "write out to tsv format...\n",
      "tsv format dataset done\n",
      "process annotation file...\n",
      "annotation file done\n",
      "processing each medical file\n",
      "['1001\\t24\\tEpisode No:  88Y206206L\\tIDNUM:88Y206206L\\n', '1001\\t36\\t8892062.BPL\\tMEDICALRECORD:8892062.BPL\\n', '1001\\t65\\tVatterott, Jerrie CLARENCE\\tPATIENT:Vatterott, Jerrie CLARENCE\\n', '1001\\t92\\tLab No:  88Y20620,88Y20620\\tIDNUM:88Y20620\\\\nIDNUM:88Y20620\\n', '1001\\t99\\tExeter\\tSTREET:Exeter\\n', '1001\\t139\\tDECEPTION BAY  Northern Territory  6845\\tCITY:DECEPTION BAY\\\\nSTATE:Northern Territory\\\\nZIP:6845\\n', '1001\\t162\\tSpecimen: Fluid,Tissue\\tPHI:Null\\n', '1001\\t181\\tD.O.B:  15/11/2004\\tDATE:15/11/2004=>2004-11-15\\n', '1001\\t189\\tSex:  F\\tPHI:Null\\n', '1001\\t215\\tCollected: 20/5/2064 at :\\tDATE:20/5/2064=>2064-05-20\\n']\n",
      "All medical file done\n",
      "write out to tsv format...\n",
      "tsv format dataset done\n"
     ]
    }
   ],
   "source": [
    "anno_info_path = r\"./content/First_Phase_Release/answer.txt\"\n",
    "report_folder = r\"./content/First_Phase_Release/First_Phase_Text_Dataset\"\n",
    "tsv_output_path = './train1.tsv'\n",
    "generate_annotated_medical_report_parallel(anno_info_path, report_folder, tsv_output_path, num_processes=4)\n",
    "\n",
    "anno_info_path = r\"./content/Second_Phase_Dataset/answer.txt\"\n",
    "report_folder = r\"./content/Second_Phase_Dataset/Second_Phase_Text_Dataset\"\n",
    "tsv_output_path = './train2.tsv'\n",
    "generate_annotated_medical_report_parallel(anno_info_path, report_folder, tsv_output_path, num_processes=4)\n",
    "\n",
    "anno_info_path = r\"./content/answer.txt\"\n",
    "report_folder = r\"./content/First_Phase_Release/Validation_Release/\"\n",
    "tsv_output_path = './valid.tsv'\n",
    "generate_annotated_medical_report_parallel(anno_info_path, report_folder, tsv_output_path, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phase_path = r'./content/opendid_test'\n",
    "valid_out_file_path = './test.tsv'\n",
    "test_txts = list(map(lambda x:os.path.join(test_phase_path , x) , os.listdir(test_phase_path)))\n",
    "test_txts = sorted(test_txts)\n",
    "valid_data = process_valid_data(test_txts , valid_out_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 處理空字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {1:'./content/First_Phase_Release/First_Phase_Text_Dataset', \n",
    "#  2:'./content/Second_Phase_Dataset/Second_Phase_Text_Dataset',\n",
    "#  3:'./content/opendid_test/', \n",
    "#  4:'./content/First_Phase_Release/Validation_Release'} \n",
    "\n",
    "df1 = pd.read_csv('./train1.tsv', header=None, delimiter='\\t').rename(columns={0:'file', 1:'start_id', 2:'sentence', 3:'label'})\n",
    "df1['sentence'] = df1.sentence.fillna('')\n",
    "df1['source'] = 1\n",
    "\n",
    "df2 = pd.read_csv('./train2.tsv', header=None, delimiter='\\t').rename(columns={0:'file', 1:'start_id', 2:'sentence', 3:'label'})\n",
    "df2['sentence'] = df2.sentence.fillna('')\n",
    "df2['source'] = 2\n",
    "\n",
    "train = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "train[['source', 'file', 'start_id','sentence','label']].to_csv('./train_pp.tsv', index=False, sep='\\t')\n",
    "\n",
    "valid = pd.read_csv('./valid.tsv', header=None, delimiter='\\t').rename(columns={0:'file', 1:'start_id', 2:'sentence', 3:'label'})\n",
    "valid['source'] = 4\n",
    "valid['sentence'] = valid.sentence.fillna('')\n",
    "valid[['source', 'file', 'start_id','sentence','label']].to_csv('./valid_pp.tsv', index=False, sep='\\t')\n",
    "\n",
    "test = pd.read_csv('./test.tsv', header=None, delimiter='\\t').rename(columns={0:'file', 1:'start_id', 2:'sentence'})\n",
    "test['source'] = 3\n",
    "test['sentence'] = test.sentence.fillna('')\n",
    "\n",
    "\n",
    "test[['source', 'file', 'start_id','sentence']].to_csv('./test_pp.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_pp.tsv', delimiter='\\t')\n",
    "valid_df = pd.read_csv('./valid_pp.tsv', delimiter='\\t')\n",
    "test_df = pd.read_csv('./test_pp.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>start_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>Episode No:  09F016547J</td>\n",
       "      <td>IDNUM:09F016547J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>091016.NMT</td>\n",
       "      <td>MEDICALRECORD:091016.NMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>SIZAR, HOWARD</td>\n",
       "      <td>PATIENT:SIZAR, HOWARD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source file  start_id                 sentence                     label\n",
       "0       1   10        25  Episode No:  09F016547J          IDNUM:09F016547J\n",
       "1       1   10        36               091016.NMT  MEDICALRECORD:091016.NMT\n",
       "2       1   10        52            SIZAR, HOWARD     PATIENT:SIZAR, HOWARD"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>start_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1001</td>\n",
       "      <td>24</td>\n",
       "      <td>Episode No:  88Y206206L</td>\n",
       "      <td>IDNUM:88Y206206L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1001</td>\n",
       "      <td>36</td>\n",
       "      <td>8892062.BPL</td>\n",
       "      <td>MEDICALRECORD:8892062.BPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1001</td>\n",
       "      <td>65</td>\n",
       "      <td>Vatterott, Jerrie CLARENCE</td>\n",
       "      <td>PATIENT:Vatterott, Jerrie CLARENCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  file  start_id                    sentence  \\\n",
       "0       4  1001        24     Episode No:  88Y206206L   \n",
       "1       4  1001        36                 8892062.BPL   \n",
       "2       4  1001        65  Vatterott, Jerrie CLARENCE   \n",
       "\n",
       "                                label  \n",
       "0                    IDNUM:88Y206206L  \n",
       "1           MEDICALRECORD:8892062.BPL  \n",
       "2  PATIENT:Vatterott, Jerrie CLARENCE  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file</th>\n",
       "      <th>start_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1097</td>\n",
       "      <td>0</td>\n",
       "      <td>433475.RDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1097</td>\n",
       "      <td>12</td>\n",
       "      <td>Timmins, ELDEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1097</td>\n",
       "      <td>27</td>\n",
       "      <td>43J47561,43J47561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  file  start_id           sentence\n",
       "0       3  1097         0         433475.RDC\n",
       "1       3  1097        12     Timmins, ELDEN\n",
       "2       3  1097        27  43J47561,43J47561"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
